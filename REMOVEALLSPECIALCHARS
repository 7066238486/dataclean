from pyspark.sql import *
from pyspark.sql.functions import *
import re

spark = SparkSession.builder.master("local[*]").appName("test").getOrCreate()
sc = spark.sparkContext
data="C:\\bigdata\\BankReviews.csv"
df=spark.read.format('csv').option('header','true').option('inferSchema','true').load(data)
df1=df.withColumn('Date',to_date(df.Date,'d-M-yyyy'))
df2=df1.select([col(x).alias(x.lower()) for x in df1.columns])
df3 = df2.withColumn('replace_reviews',regexp_replace("reviews", "[^a-zA-Z ]+", "")).drop("reviews")\
    .na.fill("no review",["replace_reviews"]).withColumn("reviews_replaced",lower(col("replace_reviews")))\
    .withColumn("stars",lower(col("stars")))\
    .withColumn("bankname",lower(col("bankname")))\
    .drop("replace_reviews")
ndf=df3.where(col("reviews_replaced")=="no review")
ndf.show()
print("no of null values are :",ndf.count())
# ndf.show(ndf.count(),False)
ndf.printSchema()


------------------------------------------------------------------------------------------------------

# adhar masking UDF
def mask(num):
    msk=re.sub(r"[0-9]{8}",'########',num)
    return msk
masking=udf(mask)

# adhar masking udf  --------------
data='C:\\Users\\Dell\\Downloads\\adhar1.csv'
df=spark.read.format('csv').option('header','true').load(data) # .option('inferSchema','true')
# Adhar masking with SQL
df.createOrReplaceTempView('emp')
ndf=spark.sql('select adhar_no,replace(adhar_no,substr(adhar_no,1,8),"########") as mask_adhar from emp')
# Adhar masking with spark
# ndf=df.withColumn('mask_adhar',masking('adhar_no'))
ndf.show()
ndf.printSchema()
